{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_tensor_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPBHDJ7QazPc"
      },
      "source": [
        "# 为什么要学习Pytorch？\n",
        "pytorch是基于Python的科学计算包。\n",
        "\n",
        "\n",
        "*   作为NumPy的替代品，可以使用GPU的强大计算能力\n",
        "*   提供最大的灵活性和高速的深度学习研究平台\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voFRJgQKbN9i"
      },
      "source": [
        "## Tensors(张量)\n",
        "\n",
        "\n",
        "*   张量的使用和Numpy中的ndarrays很类似, 区别在于张量可以在GPU或其它专用硬件上运行, 这样可以得到更快的加速效果\n",
        "*   张量如同数组和矩阵一样, 是一种特殊的数据结构。在PyTorch中, 神经网络的输入、输出以及网络的参数等数据, 都是使用张量来进行描述。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2cSWAwZcZQl"
      },
      "source": [
        "### 张量初始化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqoZI6TZaFq3",
        "outputId": "51e4a232-13a5-4506-d602-ae14c04ac8ba"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. 直接生成张量\n",
        "data = [1,2],[3,4]\n",
        "x_data = torch.tensor(data)\n",
        "print(x_data)\n",
        "\n",
        "# 2. 通过Numpy数组来生成张量\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(x_np)\n",
        "\n",
        "# 3. 通过已有的张量来生成新的张量\n",
        "#   新的张量将继承已有张量的数据属性(结构、类型), 也可以重新指定新的数据类型\n",
        "x_ones = torch.ones_like(x_data)\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "# 4. 通过指定数据维度来生成张量\n",
        "#   shape是元组类型, 用来描述张量的维数, 下面3个函数通过传入shape来指定生成张量的维数\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.2429, 0.9053],\n",
            "        [0.6666, 0.3372]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.7562, 0.0888, 0.9699],\n",
            "        [0.0447, 0.2522, 0.4461]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGaXouGkhLya"
      },
      "source": [
        "### 张量属性\n",
        "从张量属性我们可以得到张量的维数、数据类型以及它们所存储的设备(CPU或GPU)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO3MSNwYhQ-_",
        "outputId": "ef785a2c-2d40-4a90-ed34-eda25f6c1e5e"
      },
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"shape of tensor:{tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of tensor:torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlNjmKNcjBvd"
      },
      "source": [
        "### 张量运算\n",
        "有超过100种张量相关的运算操作, 例如转置、索引、切片、数学运算、线性代数、随机采样等。更多的运算可以在这里查看。\n",
        "\n",
        "所有这些运算都可以在GPU上运行(相对于CPU来说可以达到更高的运算速度)。如果你使用的是Google的Colab环境, 可以通过 Edit > Notebook Settings 来分配一个GPU使用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYbzIwFajA_e",
        "outputId": "8e0150ba-d6e0-4569-eb31-59c95997e10b"
      },
      "source": [
        "# 1. 张量的索引和切片\n",
        "tensor = torch.ones(4,4)\n",
        "tensor[:,1] = 0      # 将第1列(从0开始)的数据全部赋值为0\n",
        "print(tensor)\n",
        "\n",
        "# 2. 张量的拼接\n",
        "#   可以通过torch.cat方法将一组张量按照指定的维度进行拼接, 也可以参考torch.stack方法。这个方法也可以实现拼接操作, 但和torch.cat稍微有点不同。\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)\n",
        "\n",
        "# 3. 张量的乘积和矩阵乘法\n",
        "print(f\"tensor.mul(tensor): \\n{tensor.mul(tensor)}\")\n",
        "# 相当于\n",
        "print(f\"tensor * tensor: \\n {tensor * tensor}\")\n",
        "\n",
        "# 4. 自动赋值运算\n",
        "#   自动赋值运算通常在方法后加\"_\"\n",
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
            "tensor.mul(tensor): \n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor * tensor: \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}